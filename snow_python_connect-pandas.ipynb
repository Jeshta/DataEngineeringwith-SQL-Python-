{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#--1 - Strating with pandas \n",
    "#Reading csv file  , setting index while reading , reading data from dataframes\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "df= pd.read_csv(\"C:/Users/Sneha/documents/pandas-demo/survey_results_public.csv\")\n",
    "df_stack= pd.read_csv(\"C:/Users/Sneha/documents/pandas-demo/survey_results_public.csv\")\n",
    "schema_df = pd.read_csv(\"C:/Users/Sneha/documents/pandas-demo/survey_results_schema.csv\", index_col='Column')\n",
    "\n",
    "#reading data from dataframes\n",
    "\n",
    "schema_df\n",
    "df\n",
    "\n",
    "df.head(10)\n",
    "df.tail(10)\n",
    "\n",
    "df.shape #returns number of rows and columns \n",
    "\n",
    "df.columns #If we want to see all the columns\n",
    "\n",
    "#setting option to display all the rows and columns\n",
    "\n",
    "pd.set_option('display.max_columns', 85)\n",
    "pd.set_option('display.max_rows', 85)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    Schafer\n",
       "1        Doe\n",
       "Name: last, dtype: object"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#lesson 2 -DAtaframes , accessing rows and columns\n",
    "\n",
    "person = {\n",
    "    \"first\": \"Corey\", \n",
    "    \"last\": \"Schafer\", \n",
    "    \"email\": \"CoreyMSchafer@gmail.com\"\n",
    "}\n",
    "\n",
    "#storing mutile values for single key (acts as column and therir values)\n",
    "people = {\n",
    "    \"first\": [\"Corey\", 'Jane', 'John'], \n",
    "    \"last\": [\"Schafer\", 'Doe', 'Doe'], \n",
    "    \"email\": [\"CoreyMSchafer@gmail.com\", 'JaneDoe@email.com', 'JohnDoe@email.com']\n",
    "}\n",
    "\n",
    "#convert dict to dataframe\n",
    "person_df=pd.DataFrame(people)\n",
    "\n",
    "#note: Data frams is a container of series objects \n",
    "\n",
    "#Accesing single column\n",
    "\n",
    "person_df['first'] \n",
    "\n",
    "#Accesing multiple columns column - we pass list of column names\n",
    "\n",
    "person_df[['first','last'] ]\n",
    "\n",
    "\n",
    "\n",
    "#Accesing both rows & columns , we can use loc and iloc ..loc uses labels to get the rows where as iloc uses integer location , indexes to access rows\n",
    "\n",
    "#Access first row and getting specific columnmae with loc  (Always prefer loc than iloc)\n",
    "\n",
    "person_df.loc[0, 'last'] \n",
    "\n",
    "#Access multiple  rows and getting specific columnmae\n",
    "\n",
    "\n",
    "person_df.loc[[0,1,2], ['first','last']]  # 2 row is inclusive \n",
    "\n",
    "person_df.loc[0:2, ['first','last']]  # we can use slicing as well , 2 row is inclusive \n",
    "\n",
    "\n",
    "#Access first row and getting specific columnmae with iloc  (Always prefer loc than iloc)\n",
    "\n",
    "person_df.iloc[[0, 1], 1] #it's a location based index , we cant pass column name , instead we can pass only index value \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>email</th>\n",
       "      <th>first</th>\n",
       "      <th>last</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>CoreyMSchafer@gmail.com</td>\n",
       "      <td>Corey</td>\n",
       "      <td>Schafer</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>JaneDoe@email.com</td>\n",
       "      <td>Jane</td>\n",
       "      <td>Doe</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>JohnDoe@email.com</td>\n",
       "      <td>John</td>\n",
       "      <td>Doe</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                     email  first     last\n",
       "0  CoreyMSchafer@gmail.com  Corey  Schafer\n",
       "1        JaneDoe@email.com   Jane      Doe\n",
       "2        JohnDoe@email.com   John      Doe"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#lesson 3 -indexes -- how to set, reset & use indexes (Always recommens to set indexe on unique column , by default we have row number as index)\n",
    "\n",
    "\n",
    "\n",
    "people = {\n",
    "    \"first\": [\"Corey\", 'Jane', 'John'], \n",
    "    \"last\": [\"Schafer\", 'Doe', 'Doe'], \n",
    "    \"email\": [\"CoreyMSchafer@gmail.com\", 'JaneDoe@email.com', 'JohnDoe@email.com']\n",
    "}\n",
    "\n",
    "#convert dict to dataframe\n",
    "person_df=pd.DataFrame(people)\n",
    "\n",
    "#reseting index if we have one \n",
    "\n",
    "#person_df.reset_index(inplace=True)\n",
    "\n",
    "#setting index\n",
    "\n",
    "person_df.set_index('email', inplace=True)\n",
    "\n",
    "#person_df['email'] #e cann access the column we set for index by using person_df['email'], person_df.index['CoreyMSchafer@gmail.com']\n",
    "#we can only use indecies to quert the index column like below\n",
    "person_df.index[0]\n",
    "person_df.index[0:1]\n",
    "\n",
    "\n",
    "#resetting index\n",
    "\n",
    "person_df.reset_index(inplace=True)\n",
    "person_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>first</th>\n",
       "      <th>last</th>\n",
       "      <th>email</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Jane</td>\n",
       "      <td>Doe</td>\n",
       "      <td>JaneDoe@email.com</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>John</td>\n",
       "      <td>Doe</td>\n",
       "      <td>JohnDoe@email.com</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  first last              email\n",
       "1  Jane  Doe  JaneDoe@email.com\n",
       "2  John  Doe  JohnDoe@email.com"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "#lesson 4 --filtering , using conditionals to filter out rows and columns\n",
    "import pandas as pd\n",
    "people = {\n",
    "    \"first\": [\"Corey\", 'Jane', 'John'], \n",
    "    \"last\": [\"Schafer\", 'Doe', 'Doe'], \n",
    "    \"email\": [\"CoreyMSchafer@gmail.com\", 'JaneDoe@email.com', 'JohnDoe@email.com']\n",
    "}\n",
    "\n",
    "#convert dict to dataframe\n",
    "person_df=pd.DataFrame(people)\n",
    "person_df\n",
    "#get the rows whose last name is Doe \n",
    "filt = (person_df['last']=='Doe')\n",
    "filt  #it reutune boolean True or Flase based on condition \n",
    "person_df[filt] # we pass it to dataframe to get the actual rows and columns \n",
    "\n",
    "#we can alslo use negate to get the non matching result\n",
    "\n",
    "#filt.loc[~filt, 'email']\n",
    "\n",
    "# #another approch , we can combine above two stpes into single step \n",
    "\n",
    "filt_res =person_df[(person_df['last']=='Doe')]\n",
    "filt_res\n",
    "\n",
    "# #based on multiple conditions  , get the rows whose last name is Doeand first name is John \n",
    "\n",
    "filt_res =person_df[(person_df['last']=='Doe') & (person_df['first']=='John')]\n",
    "filt_res\n",
    "\n",
    "# #based on multiple conditions  , get the rows whose last name is Doeand first name is John  but this tiome only select email column \n",
    "\n",
    "filt_res =person_df[(person_df['last']=='Doe') & (person_df['first']=='John')]\n",
    "filt_res['email'] # or we can use below \n",
    "filt_res.loc[:,'email']\n",
    "\n",
    "# #instead of having two many condiitons on single column we can also pass in list to filter out rows\n",
    "\n",
    "# #ex: countries=['India','SA', 'Aus','NZ','srlanka']\n",
    "# #filt=df['country'].isin(countries)\n",
    "# #df[filt]\n",
    "\n",
    "# #string methods which can come in handy , similar to like operator in SQL\n",
    "\n",
    "#retreive rows where email contains \"email\" instead of gmail\n",
    "filt = person_df['email'].str.contains('email', na=False)\n",
    "person_df[filt] # or \n",
    "person_df.loc[filt] # or \n",
    "\n",
    "filt = person_df[person_df['email'].str.contains('email', na=False)]\n",
    "filt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    Chris\n",
       "1     Mary\n",
       "2     John\n",
       "Name: first_name, dtype: object"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#lesson 5 --updating  rows and columns\n",
    "\n",
    "import pandas as pd\n",
    "people = {\n",
    "    \"first\": [\"Corey\", 'Jane', 'John'], \n",
    "    \"last\": [\"Schafer\", 'Doe', 'Doe'], \n",
    "    \"email\": [\"CoreyMSchafer@gmail.com\", 'JaneDoe@email.com', 'JohnDoe@email.com']\n",
    "}\n",
    "\n",
    "#convert dict to dataframe\n",
    "person_df=pd.DataFrame(people)\n",
    "person_df\n",
    "\n",
    "#For columns - below are few usecase\n",
    "\n",
    "#how to rename all columns \n",
    "\n",
    "person_df.columns =['first_name','last_name','email'] # teduois if we have hundreds of columns \n",
    "person_df\n",
    "\n",
    "#how to rename specific column, we use rename method # rename email to email-id\n",
    "\n",
    "person_df.rename(columns = {\"email\": 'email-id'},inplace=True) \n",
    "person_df\n",
    "\n",
    "\n",
    "#connvert all column to uppercase to lower case or viceversa\n",
    "\n",
    "for col_name in person_df.columns:\n",
    "    #person_df.columns=col_name.upper() #there is issue with this code, my try\n",
    "    person_df.rename(columns={col_name: col_name.upper()}, inplace=True)\n",
    "\n",
    "#person_df\n",
    "\n",
    "\n",
    "\n",
    "for col_name in person_df.columns:\n",
    "    #person_df.columns=col_name.upper() #there is issue with this code, my try\n",
    "    person_df.rename(columns={col_name: col_name.lower()}, inplace=True)\n",
    "\n",
    "person_df\n",
    "\n",
    "#same above logic we can impletemt using list comprehension \n",
    "\n",
    "# person_df.columns= [col_name.upper() for col_name in person_df.columns]\n",
    "# person_df\n",
    "\n",
    "#Relcae blnak space to' _' in column names  \n",
    "person_df.columns= person_df.columns.str.replace('-','_')\n",
    "person_df\n",
    "\n",
    "#update rows\n",
    "\n",
    "person_df.loc[2] = ['John', 'Smith', 'JohnSmith@email.com'] #update complet row for all columns \n",
    "person_df.loc[2, ['last_name', 'email_id']] = ['Doe', 'JohnDoe@email.com'] # update row for specific column \n",
    "person_df\n",
    "\n",
    "\n",
    "\n",
    "#update single row based on condition  , update Johns last_name to smith \n",
    "\n",
    "filt =person_df['first_name']=='John'\n",
    "person_df.loc[filt,'last_name']='smith'  #use loc, apply filter on my rows and upadte the last name \n",
    "person_df\n",
    "\n",
    "#how do we update multiple rows of data - #concepts of apply , map ,applymap, replace \n",
    "\n",
    "\n",
    "#apply is used for calling a function on our values . apply can either be used on a series of objects or dataframes\n",
    "\n",
    "#convert all emailid values to lower case \n",
    "person_df['email_id'] = person_df['email_id'].str.lower()\n",
    "person_df\n",
    "#we can alsp acheive above by using apply \n",
    "\n",
    "#apply is used for calling a function on our values . apply can either be used on a series objects or dataframes\n",
    "#when we use this on a series , it will apply a function on every value in our series. \n",
    "#Ex: if we want to find the legth of all our email id  but we can also use this to update values as well\n",
    "\n",
    "\n",
    "#---APPLY-----\n",
    "#below exampls shows how apply works on series objects\n",
    "\n",
    "#length of email column values\n",
    "person_df['email_id'].apply(len)\n",
    "\n",
    "\n",
    "#convert all emailid values to upper case  by using udf \n",
    "\n",
    "def update_email(email_id):\n",
    "    return email_id.upper()\n",
    "\n",
    "person_df['email_id']= person_df['email_id'].apply(update_email)\n",
    "person_df\n",
    "\n",
    "# we can also use lambda functions to acheive above result instead of udf-- basically it is anonymous function without specific name \n",
    "\n",
    "person_df['email_id']= person_df['email_id'].apply(lambda x: x.lower())\n",
    "person_df\n",
    "\n",
    "#below exampls shows how apply works on Dataframes\n",
    "#When we run apply on a dataframe, it runs a function on each row and column of that dataframe\n",
    "person_df.apply(len) #it doesnt apply length fun on every value in teh dataframe , \n",
    "#it acutally applies length function on each series in a dataframe..basically it returns number of rows for each column \n",
    "\n",
    "person_df.apply(len, axis= 'columns')  #by default is is rows \n",
    "\n",
    "#summary\n",
    "#running apply on a series, applies function to every value in the series\n",
    "#running apply on a df, applies a function to every series in a dataframe ( it doesnt apply to every individual in a dataframe) \n",
    "\n",
    "\n",
    "#--APPLYMAP\n",
    "#Here come applymap, where it runs on every element in a dataframe \n",
    "#Applymap works only for a dataframe\n",
    "\n",
    "\n",
    "person_df.applymap(len) # it applies len function on every individual element in a dataframe\n",
    "\n",
    "#ex: if we want to have all teh values in the lowercase then \n",
    "\n",
    "person_df.applymap(str.lower) #if we have numerical data it will throws an error as string methods doesnt work on numerical data\n",
    "\n",
    "\n",
    "#-----MAP--------it works similar to case statement in SQL\n",
    "#Now , we have map method , only works on a series\n",
    "#MAP is used for substituting value in a series with other value \n",
    "\n",
    "#Ex: lest say if we want to substitute a couple of our first names\n",
    "person_df['first_name'].map({'Corey': 'Chris', 'Jane': 'Mary'})\n",
    "# NOTE: the values we dint substitute becomes a NaN value (not a number value), basically it will get rid of other names\n",
    "\n",
    "\n",
    "#but what if i want to have the other names as it is . without converting to NaN , the we can use replace method\n",
    "\n",
    "#REPLACE --it works similar to case statement in SQL\n",
    "person_df['first_name'].replace({'Corey': 'Chris', 'Jane': 'Mary'})\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>email</th>\n",
       "      <th>fullname</th>\n",
       "      <th>first</th>\n",
       "      <th>last</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>CoreyMSchafer@gmail.com</td>\n",
       "      <td>Corey Schafer</td>\n",
       "      <td>Corey</td>\n",
       "      <td>Schafer</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>JaneDoe@email.com</td>\n",
       "      <td>Jane Doe</td>\n",
       "      <td>Jane</td>\n",
       "      <td>Doe</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>JohnDoe@email.com</td>\n",
       "      <td>John Doe</td>\n",
       "      <td>John</td>\n",
       "      <td>Doe</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                     email       fullname  first     last\n",
       "0  CoreyMSchafer@gmail.com  Corey Schafer  Corey  Schafer\n",
       "1        JaneDoe@email.com       Jane Doe   Jane      Doe\n",
       "2        JohnDoe@email.com       John Doe   John      Doe"
      ]
     },
     "execution_count": 90,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#lesson 6 --Add/ remove rows, columns in a dataframe\n",
    "\n",
    "\n",
    "people = {\n",
    "    \"first\": [\"Corey\", 'Jane', 'John'], \n",
    "    \"last\": [\"Schafer\", 'Doe', 'Doe'], \n",
    "    \"email\": [\"CoreyMSchafer@gmail.com\", 'JaneDoe@email.com', 'JohnDoe@email.com']\n",
    "}\n",
    "\n",
    "#convert dict to dataframe\n",
    "person_df=pd.DataFrame(people)\n",
    "person_df\n",
    "\n",
    "\n",
    "#Adding columns\n",
    "#Add new column called full name by adding first & last\n",
    "\n",
    "person_df['fullname']= person_df['first'] + \" \" + person_df['last']\n",
    "person_df\n",
    "\n",
    "#we can also use apply method to add new column \n",
    "\n",
    "#Dropping columns\n",
    "\n",
    "person_df.drop(columns=['first','last'],inplace= True)\n",
    "person_df\n",
    "\n",
    "#split full name to fname and lname\n",
    "\n",
    "person_df['fullname'].str.split(' ',expand= True)\n",
    "#to store those two values in seperate column we use expand = True\n",
    "# to set those to columns \n",
    "\n",
    "person_df[['first','last']]= person_df['fullname'].str.split(' ',expand= True)\n",
    "person_df\n",
    "\n",
    "\n",
    "\n",
    "#Adding a single row to dataframe \n",
    "df = pd.concat([person_df, pd.DataFrame({'first': ['Tony'],'last':['shark']})])\n",
    "df # pints to remebr - Pas svalues in list inorder to avoid index issue \n",
    "\n",
    "#append a dataframe to existing dataframe \n",
    "\n",
    "\n",
    "person_df2 = pd.DataFrame({\n",
    "  'A': [7, 8, 9],\n",
    "  'B': [10, 11, 12]\n",
    "})\n",
    "\n",
    "# Appending person_df2 to person_df\n",
    "df = pd.concat([person_df, person_df2], ignore_index=True, sort=False)\n",
    "df\n",
    "\n",
    "#removing rows  or dropping rows\n",
    "\n",
    "#Dropping rows based on index\n",
    "\n",
    "df.index #shows the index\n",
    "df.drop(index=5, inplace=True)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "#dropping the rows based on condition \n",
    "\n",
    "# get records with last name \"Doe\"\n",
    "\n",
    "filt = df['last']=='Doe'\n",
    "df[filt]\n",
    "\n",
    "# get records with last name is not \"Doe\"\n",
    "\n",
    "filt = df['last']=='Doe'\n",
    "#df[~filt].index #to get the index\n",
    "\n",
    "df.drop(index=df[~filt].index)\n",
    "df\n",
    "\n",
    "\n",
    "#clean ,self practice - remove all newlyadded rows and columns \n",
    "\n",
    "df.drop(columns=['A','B'],inplace=True)\n",
    "df\n",
    "\n",
    "#remove rows with  NaN\n",
    "#Using DataFrame.dropna() method drop all rows that have NAN/none.\n",
    "df=df.dropna()\n",
    "df\n",
    "\n",
    "# Filter NAN Data selection column of strings by not operator.\n",
    "# ex: df2 = df[~pd.isnull(df['Courses'])]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>first</th>\n",
       "      <th>last</th>\n",
       "      <th>email</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Corey</td>\n",
       "      <td>Schafer</td>\n",
       "      <td>CoreyMSchafer@gmail.com</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Jane</td>\n",
       "      <td>Doe</td>\n",
       "      <td>JaneDoe@email.com</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>John</td>\n",
       "      <td>Doe</td>\n",
       "      <td>JohnDoe@email.com</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Adam</td>\n",
       "      <td>Doe</td>\n",
       "      <td>A@email.com</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   first     last                    email\n",
       "0  Corey  Schafer  CoreyMSchafer@gmail.com\n",
       "1   Jane      Doe        JaneDoe@email.com\n",
       "2   John      Doe        JohnDoe@email.com\n",
       "3   Adam      Doe              A@email.com"
      ]
     },
     "execution_count": 93,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#lesson 7 --sorting data , on single columns or multiple columns \n",
    "people = {\n",
    "    'first': ['Corey', 'Jane', 'John', 'Adam'], \n",
    "    'last': ['Schafer', 'Doe', 'Doe', 'Doe'], \n",
    "    'email': ['CoreyMSchafer@gmail.com', 'JaneDoe@email.com', 'JohnDoe@email.com', 'A@email.com']\n",
    "}\n",
    "import pandas as pd\n",
    "df = pd.DataFrame(people)\n",
    "df\n",
    "\n",
    "#sort the data base don single column  , by default ascending \n",
    "\n",
    "df.sort_values(by='last', ascending=False)\n",
    "\n",
    "\n",
    "#sort the data base don multiple  columns  , by default ascending \n",
    "\n",
    "df.sort_values(by=['last','first'], ascending=False)\n",
    "\n",
    "\n",
    "#sort the data base don multiple  columns  , by default ascending ..but one with asc and other with desc \n",
    "\n",
    "df.sort_values(by=['last', 'first'], ascending=[False, True], inplace=True)\n",
    "df\n",
    "\n",
    "#incase if we want the changes back , we can sort it by index \n",
    "df.sort_index()\n",
    "\n",
    "#sortvalues method can also be applicable to series \n",
    "df['last'].sort_values()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "NumRespondents    391.000000\n",
       "NumKnowsPython    182.000000\n",
       "PctKnowsPython     46.547315\n",
       "Name: Japan, dtype: float64"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#lesson 8 --Grouping and Aggregating data\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "ipl_data = {'Team': ['Riders', 'Riders', 'Devils', 'Devils', 'Kings',\n",
    "   'kings', 'Kings', 'Kings', 'Riders', 'Royals', 'Royals', 'Riders'],\n",
    "   'Rank': [1, 2, 2, 3, 3,4 ,1 ,1,2 , 4,1,2],\n",
    "   'Year': [2014,2015,2014,2015,2014,2015,2016,2017,2016,2014,2015,2017],\n",
    "   'Points':[876,789,863,673,741,812,756,788,694,701,804,690]}\n",
    "df = pd.DataFrame(ipl_data)\n",
    "\n",
    "#print(df)\n",
    "\n",
    "\n",
    "#get the count of each team \n",
    "\n",
    "test=df['Team'].value_counts()\n",
    "#print(test)\n",
    "\n",
    "# Riders    4\n",
    "# Kings     3\n",
    "# Devils    2\n",
    "# Royals    2\n",
    "# kings     1\n",
    "\n",
    "#try using group by \n",
    "\n",
    "grp= df.groupby('Team')\n",
    "#print(grp['Team'].count())\n",
    "\n",
    "# Riders    4\n",
    "# Kings     3\n",
    "# Devils    2\n",
    "# Royals    2\n",
    "# kings     1\n",
    "\n",
    "\n",
    "# GROUPBY basically performs 3 steps 1)split  2) apply 3)combine\n",
    "\n",
    "#group by single coloun \n",
    "grp= df.groupby('Team')\n",
    "#print(grp) #pandas.core.groupby.generic.DataFrameGroup\n",
    "\n",
    "#print(grp.groups) #Access the groups\n",
    "#{'Devils': [2, 3], 'Kings': [4, 6, 7], 'Riders': [0, 1, 8, 11], 'Royals': [9, 10], 'kings': [5]}\n",
    "\n",
    "\n",
    "\n",
    "#group by multiple columns\n",
    "grp2= df.groupby(['Team','Year'])\n",
    "#print(grp2) #pandas.core.groupby.generic.DataFrameGroup\n",
    "\n",
    "#print(grp2.groups) #Access the groups\n",
    "#{('Devils', 2014): [2], ('Devils', 2015): [3], ('Kings', 2014): [4], ('Kings', 2016): [6], ('Kings', 2017): [7], ('Riders', 2014): [0], ('Riders', 2015): [1], ('Riders', 2016): [8], ('Riders', 2017): [11], ('Royals', 2014): [9], ('Royals', 2015): [10], ('kings', 2015): [5]\n",
    "\n",
    "#Iterating through Groups\n",
    "\n",
    "# for group_name ,group in grp2:\n",
    "#     print(group_name)\n",
    "#     print(group)\n",
    "\n",
    "\n",
    "# for group_name ,group in grp:\n",
    "#     print(group_name)\n",
    "#     print(group)\n",
    "\n",
    "#How to select partcular group ? we use get_group() method \n",
    "#By default, the groupby object has the same label name as the group name\n",
    "\n",
    "grouped = df.groupby('Year')\n",
    "#print(grouped.get_group(2014))\n",
    "\n",
    "grp= df.groupby('Team')\n",
    "#print(grp.get_group('Devils'))\n",
    "\n",
    "grp2= df.groupby(['Team','Year'])\n",
    "#print(grp2.get_group(('Devils',2014)))\n",
    "\n",
    "#we perform aggregations on each group (Apply) & then combine all the groups \n",
    "\n",
    "#An aggregated function returns a single aggregated value for each group.\n",
    "# Once the group by object is created, several aggregation operations can be performed on the grouped data.\n",
    "\n",
    "#average points for each team \n",
    "grp= df.groupby('Team')\n",
    "#print(grp['Points'].mean())\n",
    "\n",
    "#Total points for each team \n",
    "grp= df.groupby('Team')\n",
    "#print(grp['Points'].sum())\n",
    "\n",
    "#Applying Multiple Aggregation Functions at Once\n",
    "grouped = df.groupby('Team')\n",
    "#print(grouped['Points'].agg(['sum', 'mean', 'std']))  #.aff is used if we want to use multiple agg functions\n",
    "\n",
    "\n",
    "\n",
    "#we can apply agg function on series objects\n",
    "\n",
    "df_stack\n",
    "df_stack['ConvertedComp'].median()\n",
    "\n",
    "#to geg the five point summary \n",
    "\n",
    "df_stack.describe() # It will apply to all the numerical data, it will ignore NaN values\n",
    "\n",
    "#just only get median  for all teh columns \n",
    "#df_stack.median()\n",
    "\n",
    "#to geg the five point summary for one column \n",
    "df_stack['ConvertedComp'].describe()\n",
    "\n",
    "#difference between count() and value_count()\n",
    "\n",
    "#count() - totoal count (rows)\n",
    "# value_count()-- give the counts based on breakdowns\n",
    "\n",
    "#If we want to see the distribition , add normalization to value count\n",
    "df_stack\n",
    "df_stack['SocialMedia'].value_counts(normalize= True)\n",
    "\n",
    "\n",
    "#Find most popular social media site for each country \n",
    "\n",
    "grp=df_stack.groupby('Country')\n",
    "grp['SocialMedia'].value_counts()\n",
    "\n",
    "grp=df_stack.groupby('Country')\n",
    "grp['SocialMedia'].value_counts(normalize= True)\n",
    "\n",
    "\n",
    "#Find most popular social media site for each country  and query for each country --use index country \n",
    "\n",
    "grp=df_stack.groupby('Country')\n",
    "grp['SocialMedia'].value_counts(normalize= True).loc['United States']\n",
    "\n",
    "grp['SocialMedia'].value_counts(normalize= True).loc['Thailand']\n",
    "\n",
    "\n",
    "#How many people shows interest in python language based on each country\n",
    "grp=df_stack.groupby('Country')\n",
    "#grp['LanguageWorkedWith'].str.contains('python') #str functions cannot work on grp by series , insted use apply method\n",
    "\n",
    "grp['LanguageWorkedWith'].apply(lambda x :  x.str.contains('Python').sum()) #we get teh total number for each contry \n",
    "\n",
    "grp['LanguageWorkedWith'].apply(lambda x :  x.str.contains('Python').sum()).loc['United States'] #we get teh total number for each contry  #uery for specific country\n",
    "\n",
    "#above functionality we can also acheive using filtering ..\n",
    "filt = df_stack['Country'] == 'India'\n",
    "df_stack.loc[filt]['LanguageWorkedWith'].str.contains('Python').sum()\n",
    "\n",
    "\n",
    "#what % of people from each country know python\n",
    "\n",
    "country_respondents = df_stack['Country'].value_counts()\n",
    "#print(country_respondents)\n",
    "\n",
    "python_worked= grp['LanguageWorkedWith'].apply(lambda x :  x.str.contains('Python').sum())\n",
    "#print(python_worked)\n",
    "\n",
    "\n",
    "# combine = pd.concat([country_respondents,python_worked]) #by default it will combine to rows (axis=rows)\n",
    "# #print(combine.head(5))\n",
    "\n",
    "combine2 = pd.concat([country_respondents,python_worked], axis='columns',sort= False) #by default it will combine to rows (axis=rows)\n",
    "#print(combine2.head(5))\n",
    "\n",
    "# #rename more column to make more sense\n",
    "combine2.rename(columns={'count': 'NumRespondents', 'LanguageWorkedWith': 'NumKnowsPython'}, inplace=True)\n",
    "\n",
    "#print(combine2)\n",
    "\n",
    "combine2['PctKnowsPython'] = (combine2['NumKnowsPython']/combine2['NumRespondents']) * 100\n",
    "combine2.sort_values(by='PctKnowsPython', ascending=False, inplace=True)\n",
    "combine2.head(50)\n",
    "#For specifci conutry \n",
    "combine2.loc['Japan']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    Schafer\n",
       "1        Doe\n",
       "2        Doe\n",
       "3    Schafer\n",
       "4          0\n",
       "5          0\n",
       "6          0\n",
       "Name: last, dtype: object"
      ]
     },
     "execution_count": 80,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#lesson 9 --Cleaning data - CAsting datatypes and handling missing values\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "people_df = pd.DataFrame({\n",
    "    'first': ['Corey', 'Jane', 'John', 'Chris', np.nan, None, 'NA'], \n",
    "    'last': ['Schafer', 'Doe', 'Doe', 'Schafer', np.nan, np.nan, 'Missing'], \n",
    "    'email': ['CoreyMSchafer@gmail.com', 'JaneDoe@email.com', 'JohnDoe@email.com', None, np.nan, 'Anonymous@email.com', 'NA'],\n",
    "    'age': ['33', '55', '63', '36', None, None, 'Missing']\n",
    "})\n",
    "people_df\n",
    "\n",
    "#--DROPPING NULL Values\n",
    "\n",
    "#df.dropna()\n",
    "#df.isna() #Gives the boolean values True or False in a dataframe for each column\n",
    "\n",
    "\n",
    "#replace text 'NA' and 'Missing' to numpy NaN # we can also handle this while reading csv file like below \n",
    "\n",
    "people_df.replace('NA', np.nan, inplace=True)\n",
    "people_df.replace('Missing', np.nan, inplace=True)\n",
    "\n",
    "# na_vals = ['NA', 'Missing']\n",
    "# df = pd.read_csv('data/survey_results_public.csv', index_col='Respondent', na_values=na_vals) # it will replace 'NA' and 'MISSING' to Nnumpy NaN\n",
    "\n",
    "\n",
    "\n",
    "#NaN are the null values\n",
    "\n",
    "#drop athe null values based on rows\n",
    "\n",
    "people_df.dropna() #it will drop all the rows with any NaN value ( under the hood ,it takes these parameter , axis = index, how = any like below)\n",
    "\n",
    "people_df.dropna(axis = 'index',how = 'any')\n",
    "\n",
    "#If we want to  drop teh records, if we have NaN for all teh columns \n",
    "\n",
    "people_df.dropna(axis = 'index',how = 'all')\n",
    "\n",
    "# if we want to drop the records based on NaN for specific columns\n",
    "\n",
    "people_df.dropna(axis = 'index',how = 'any', subset=['last'])\n",
    "\n",
    "people_df.dropna(axis = 'index',how = 'all', subset=['last','first'])  #both column should contal NaN\n",
    "\n",
    "\n",
    "#drop athe null values based on columns  # set axis to column \n",
    "\n",
    "#It will drop teh columns that all have missing values\n",
    "\n",
    "people_df.dropna(axis = 'columns',how = 'all')\n",
    "\n",
    "people_df.dropna(axis = 'columns',how = 'any')\n",
    "\n",
    "\n",
    "#--FILL MISSING VALUES\n",
    "\n",
    "people_df.fillna('fillingvalue')\n",
    "\n",
    "#replace NaN with 0\n",
    "people_df.fillna(0)\n",
    "\n",
    "#fill NaN for specific columns \n",
    "people_df['last'].fillna(0)\n",
    "\n",
    "#Casting Datatypes\n",
    "\n",
    "#Get the average age in sample dataframe \n",
    "\n",
    "#check if teh age is numerical data\n",
    "#check datatypes\n",
    "people_df.dtypes\n",
    "\n",
    "# first    object\n",
    "# last     object\n",
    "# email    object\n",
    "# age      object --object is nothing but a string , we need to convert to numerical\n",
    "\n",
    "#try to conver to int\n",
    "#people_df['age']= people_df['age'].astype(int) #it throws an error because 'age' column contains NaN and those under the hood casted as float\n",
    "\n",
    "#try to conver to float\n",
    "\n",
    "people_df['age']= people_df['age'].astype(float) \n",
    "\n",
    "people_df.dtypes\n",
    "\n",
    "people_df['age'].mean() #46.75 \n",
    "\n",
    "#we can apply astype to dataframe aswell\n",
    "#people_df.astype(float) # it will convert all teh data types to float , incase if the values are numeric\n",
    "\n",
    "\n",
    "\n",
    "# to find unique value in a column \n",
    "people_df['first'].unique()\n",
    "\n",
    "# df['YearsCode'].unique()\n",
    "# df['YearsCode'].replace('Less than 1 year', 0, inplace=True)\n",
    "# df['YearsCode'].replace('More than 50 years', 51, inplace=True)\n",
    "# df['YearsCode'] = df['YearsCode'].astype(float)\n",
    "# df['YearsCode'].mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Date\n",
       "2017-07-01    279.99\n",
       "2017-07-02    293.73\n",
       "2017-07-03    285.00\n",
       "2017-07-04    282.83\n",
       "2017-07-05    274.97\n",
       "2017-07-06    275.00\n",
       "2017-07-07    266.97\n",
       "2017-07-08    249.50\n",
       "2017-07-09    253.31\n",
       "2017-07-10    240.33\n",
       "Freq: D, Name: High, dtype: float64"
      ]
     },
     "execution_count": 111,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#lesson 10 --working with dates and timeseries data\n",
    "\n",
    "from datetime import datetime\n",
    "import pandas as pd\n",
    "\n",
    "df_eth = pd.read_csv(\"C:/Users/Sneha/documents/pandas-demo/ETH_1h.csv\")\n",
    "df_eth\n",
    "\n",
    "#get the datatypes and check for date relate dcolumns\n",
    "df_eth.dtypes # so , Date column is in text ..so we need to convert to Date datatype to perform any analysis\n",
    "\n",
    "#convert to Date datatype\n",
    "\n",
    "#df_eth['Date']= pd.to_datetime(df_eth['Date']) # it thrws error saying unknow format\n",
    "df_eth['Date']= pd.to_datetime(df_eth['Date'], format = '%Y-%m-%d %I-%p')\n",
    "df_eth\n",
    "\n",
    "#df_eth['Date'].day_name() #series object doesnt have day_name method instrea we can use below on dataframe \n",
    "df_eth['Date'].dt.day_name()\n",
    "\n",
    "# for specific record \n",
    "df_eth.loc[0,'Date'].day_name()\n",
    "\n",
    "#create new column \n",
    "df_eth['DayOfWeek'] = df_eth['Date'].dt.day_name()\n",
    "#df_eth\n",
    "\n",
    "# we can also find min , max , delta of teh dates \n",
    "df_eth['Date'].min()\n",
    "df_eth['Date'].max()\n",
    "df_eth['Date'].max() - df_eth['Date'].min()\n",
    "\n",
    "#we can alos filter records based on dates (specific data)\n",
    "\n",
    "filt = df_eth['Date']=='2020-02-01'\n",
    "df_eth[filt]\n",
    "\n",
    "filt = (df_eth['Date'] > '2020-02-01') &  (df_eth['Date'] < '2020-05-01')\n",
    "df_eth[filt]\n",
    "\n",
    "# Another example\n",
    "# filt = (df['Date'] >= pd.to_datetime('2019-01-01')) & (df['Date'] < pd.to_datetime('2020-01-01'))\n",
    "# df.loc[filt]\n",
    "\n",
    "#set date column to index and the column is unique\n",
    "df_eth.set_index('Date',inplace=True)\n",
    "df_eth\n",
    "\n",
    "\n",
    "df_eth['2020-01':'2020-02']\n",
    "df_eth['2020-01':'2020-02']['Close'].mean()\n",
    "\n",
    "#resample\n",
    "\n",
    "highs = df_eth['High'].resample('D').max() # D for day , W for week \n",
    "highs.head(10)\n",
    "\n",
    "# If we want to convert teh date to date dataype as we are loading data thr csv. we can do that as well\n",
    "# d_parser = lambda x: datetime.strptime(x, '%Y-%m-%d %I-%p')\n",
    "# df = pd.read_csv(\"C:/Users/Sneha/documents/pandas-demo/ETH_1h.csv\", parse_dates=['Date'], date_parser=d_parser)\n",
    "# df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Date</th>\n",
       "      <th>Symbol</th>\n",
       "      <th>Open</th>\n",
       "      <th>High</th>\n",
       "      <th>Low</th>\n",
       "      <th>Close</th>\n",
       "      <th>Volume</th>\n",
       "      <th>DayOfWeek</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2020-03-13 20:00:00</td>\n",
       "      <td>ETHUSD</td>\n",
       "      <td>129.94</td>\n",
       "      <td>131.82</td>\n",
       "      <td>126.87</td>\n",
       "      <td>128.71</td>\n",
       "      <td>1940673.93</td>\n",
       "      <td>Friday</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2020-03-13 19:00:00</td>\n",
       "      <td>ETHUSD</td>\n",
       "      <td>119.51</td>\n",
       "      <td>132.02</td>\n",
       "      <td>117.10</td>\n",
       "      <td>129.94</td>\n",
       "      <td>7579741.09</td>\n",
       "      <td>Friday</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2020-03-13 18:00:00</td>\n",
       "      <td>ETHUSD</td>\n",
       "      <td>124.47</td>\n",
       "      <td>124.85</td>\n",
       "      <td>115.50</td>\n",
       "      <td>119.51</td>\n",
       "      <td>4898735.81</td>\n",
       "      <td>Friday</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2020-03-13 17:00:00</td>\n",
       "      <td>ETHUSD</td>\n",
       "      <td>124.08</td>\n",
       "      <td>127.42</td>\n",
       "      <td>121.63</td>\n",
       "      <td>124.47</td>\n",
       "      <td>2753450.92</td>\n",
       "      <td>Friday</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2020-03-13 16:00:00</td>\n",
       "      <td>ETHUSD</td>\n",
       "      <td>124.85</td>\n",
       "      <td>129.51</td>\n",
       "      <td>120.17</td>\n",
       "      <td>124.08</td>\n",
       "      <td>4461424.71</td>\n",
       "      <td>Friday</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>2020-03-13 15:00:00</td>\n",
       "      <td>ETHUSD</td>\n",
       "      <td>128.39</td>\n",
       "      <td>128.90</td>\n",
       "      <td>116.06</td>\n",
       "      <td>124.85</td>\n",
       "      <td>7378976.00</td>\n",
       "      <td>Friday</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>2020-03-13 14:00:00</td>\n",
       "      <td>ETHUSD</td>\n",
       "      <td>134.03</td>\n",
       "      <td>137.90</td>\n",
       "      <td>125.50</td>\n",
       "      <td>128.39</td>\n",
       "      <td>3733916.89</td>\n",
       "      <td>Friday</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>2020-03-13 13:00:00</td>\n",
       "      <td>ETHUSD</td>\n",
       "      <td>131.35</td>\n",
       "      <td>140.95</td>\n",
       "      <td>128.99</td>\n",
       "      <td>134.03</td>\n",
       "      <td>9582732.93</td>\n",
       "      <td>Friday</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>2020-03-13 12:00:00</td>\n",
       "      <td>ETHUSD</td>\n",
       "      <td>128.93</td>\n",
       "      <td>134.60</td>\n",
       "      <td>126.95</td>\n",
       "      <td>131.35</td>\n",
       "      <td>3906590.52</td>\n",
       "      <td>Friday</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>2020-03-13 11:00:00</td>\n",
       "      <td>ETHUSD</td>\n",
       "      <td>132.60</td>\n",
       "      <td>133.17</td>\n",
       "      <td>126.01</td>\n",
       "      <td>128.93</td>\n",
       "      <td>3311080.29</td>\n",
       "      <td>Friday</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                 Date  Symbol    Open    High     Low   Close      Volume  \\\n",
       "0 2020-03-13 20:00:00  ETHUSD  129.94  131.82  126.87  128.71  1940673.93   \n",
       "1 2020-03-13 19:00:00  ETHUSD  119.51  132.02  117.10  129.94  7579741.09   \n",
       "2 2020-03-13 18:00:00  ETHUSD  124.47  124.85  115.50  119.51  4898735.81   \n",
       "3 2020-03-13 17:00:00  ETHUSD  124.08  127.42  121.63  124.47  2753450.92   \n",
       "4 2020-03-13 16:00:00  ETHUSD  124.85  129.51  120.17  124.08  4461424.71   \n",
       "5 2020-03-13 15:00:00  ETHUSD  128.39  128.90  116.06  124.85  7378976.00   \n",
       "6 2020-03-13 14:00:00  ETHUSD  134.03  137.90  125.50  128.39  3733916.89   \n",
       "7 2020-03-13 13:00:00  ETHUSD  131.35  140.95  128.99  134.03  9582732.93   \n",
       "8 2020-03-13 12:00:00  ETHUSD  128.93  134.60  126.95  131.35  3906590.52   \n",
       "9 2020-03-13 11:00:00  ETHUSD  132.60  133.17  126.01  128.93  3311080.29   \n",
       "\n",
       "  DayOfWeek  \n",
       "0    Friday  \n",
       "1    Friday  \n",
       "2    Friday  \n",
       "3    Friday  \n",
       "4    Friday  \n",
       "5    Friday  \n",
       "6    Friday  \n",
       "7    Friday  \n",
       "8    Friday  \n",
       "9    Friday  "
      ]
     },
     "execution_count": 98,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#lesson 11 --#reading / writing data to different sources - excel , json , sql etc..\n",
    "\n",
    "# filt = (df['Country'] == 'India')\n",
    "# india_df = df.loc[filt]\n",
    "# india_df.head()\n",
    "\n",
    "# india_df.to_csv('data/modified.csv')\n",
    "# india_df.to_csv('data/modified.tsv', sep='\\t')\n",
    "# india_df.to_excel('data/modified.xlsx')\n",
    "# test = pd.read_excel('data/modified.xlsx', index_col='Respondent')\n",
    "\n",
    "# india_df.to_json('data/modified.json', orient='records', lines=True)\n",
    "# test = pd.read_json('data/modified.json', orient='records', lines=True)\n",
    "# test.head()\n",
    "\n",
    "\n",
    "# from sqlalchemy import create_engine\n",
    "# import psycopg2\n",
    "\n",
    "# engine = create_engine('postgresql://dbuser:dbpass@localhost:5432/sample_db')\n",
    "# india_df.to_sql('sample_table', engine, if_exists='replace')\n",
    "# sql_df = pd.read_sql('sample_table', engine, index_col='Respondent')\n",
    "# sql_df.head()\n",
    "\n",
    "# sql_df = pd.read_sql_query('SELECT * FROM sample_table', engine, index_col='Respondent')\n",
    "# sql_df.head()\n",
    "\n",
    "# posts_df = pd.read_json('https://raw.githubusercontent.com/CoreyMSchafer/code_snippets/master/Python/Flask_Blog/snippets/posts.json')\n",
    "# posts_df.head()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 169,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "()\n",
      "['Aj', 'Sam', 'Rob', 'Kevin', '80']\n",
      "Aj\n",
      "80\n",
      "['Aj', 'Sam']\n",
      "['Aj', 'Sam', 'Rob']\n",
      "['Aj', 'Sam', 'Rob', 'Kevin', '80', 'dk']\n",
      "['Abd', 'Aj', 'Sam', 'Rob', 'Kevin', '80', 'dk']\n",
      "['Abd', 'Aj', 'Sam', 'Rob', 'Kevin', '80', 'dk', 'Sa', 'ind', 'Aus']\n",
      "['Abd', 'Aj', 'Sam', 'Rob', 'Kevin', '80', 'Sa', 'ind', 'Aus']\n",
      "['Abd', 'Aj', 'Sam', 'Rob', 'Kevin', '80', 'Sa', 'ind']\n",
      "Aus\n",
      "['Abd', 'Aj', 'Sam', 'Rob', 'Kevin', 'Sa', 'ind']\n",
      "['Abd', 'Aj', 'Kevin', 'Rob', 'Sa', 'Sam', 'ind']\n",
      "['ind', 'Sam', 'Sa', 'Rob', 'Kevin', 'Aj', 'Abd']\n",
      "ind\n",
      "Sam\n",
      "Sa\n",
      "Rob\n",
      "Kevin\n",
      "Aj\n",
      "Abd\n",
      "(0, 'ind')\n",
      "(1, 'Sam')\n",
      "(2, 'Sa')\n",
      "(3, 'Rob')\n",
      "(4, 'Kevin')\n",
      "(5, 'Aj')\n",
      "(6, 'Abd')\n",
      "0 ind\n",
      "1 Sam\n",
      "2 Sa\n",
      "3 Rob\n",
      "4 Kevin\n",
      "5 Aj\n",
      "6 Abd\n",
      "1 ind\n",
      "2 Sam\n",
      "3 Sa\n",
      "4 Rob\n",
      "5 Kevin\n",
      "6 Aj\n",
      "7 Abd\n",
      "6\n",
      "True\n",
      "ind-Sam-Sa-Rob-Kevin-Aj-Abd\n",
      "['ind', 'Sam', 'Sa', 'Rob', 'Kevin', 'Aj', 'Abd']\n",
      "('Aj', 'Sam', 80)\n",
      "Aj\n",
      "('Aj', 'Sam', 80, 'nash', 'Sam', 80)\n",
      "{'Aj', 'nash', 80, 'dan'}\n",
      "{'Aj', 'Rod', 'Shawn'}\n",
      "{'Aj'}\n",
      "{80, 'nash', 'dan'}\n",
      "{80, 'Rod', 'Aj', 'nash', 'Shawn', 'dan'}\n",
      "['Sa', 'ind', 'Aus', 'Ban']\n"
     ]
    }
   ],
   "source": [
    "# Lists & Dictionaries\n",
    "\n",
    "#create empty list , tuples, sets   \n",
    "\n",
    "# Empty Lists\n",
    "empty_list = []\n",
    "#print(empty_list)\n",
    "\n",
    "empty_list1= list()\n",
    "#print(empty_list1)\n",
    "\n",
    "\n",
    "# # Empty Tuples\n",
    "empty_tuple = ()\n",
    "#print(empty_tuple)\n",
    "\n",
    "empty_tuple1 = tuple()\n",
    "print(empty_tuple1)\n",
    "\n",
    "# Empty dict\n",
    "empty_dict = {} \n",
    "#print(empty_dict)\n",
    "\n",
    "#Empty set \n",
    "empty_set = set()\n",
    "#print(empty_set)\n",
    "\n",
    "\n",
    "#list \n",
    "\n",
    "# create a list \n",
    "\n",
    "#it is mutable , it can hold multiple data types & it is ordered \n",
    "\n",
    "names =['Aj','Sam', 'Rob','Kevin','80'] # it can hold multiple data types & its is ordered \n",
    "print(names)\n",
    "\n",
    "#length of string\n",
    "\n",
    "len(names)\n",
    "\n",
    "# Access a list (index , negative inde)\n",
    "\n",
    "print(names[0])\n",
    "print(names[-1])\n",
    "\n",
    "# slicing list \n",
    "\n",
    "print(names[0:2])\n",
    "print(names[:-2])\n",
    "\n",
    "\n",
    "# Adding values to list  (Modifying list )\n",
    "\n",
    "names.append('dk')   # append adds value at the list \n",
    "print(names) \n",
    "\n",
    "names.insert(0,'Abd')   # insert adds value ato the specific index\n",
    "print(names) \n",
    "\n",
    "#We can aloso use + operator to combine two lists\n",
    "add =['Sa','ind','Aus']\n",
    "add2=['Ban']\n",
    "\n",
    "print(add + add2)\n",
    "\n",
    "add =['Sa','ind','Aus']\n",
    "names.extend(add)   # extend add data to list as an individual element , incase if we iuse insert then \n",
    "                    #it will add inside list as another list ,if we are trying to add multipl values\n",
    "print(names)\n",
    "\n",
    "# remove values from list \n",
    "\n",
    "# we have pop method and remove method . Difference is with pop it will remove last value and can store removed value to a variable\n",
    "\n",
    "names.remove('dk')\n",
    "print(names)\n",
    "\n",
    "removed= names.pop()\n",
    "print(names)\n",
    "print(removed)\n",
    "\n",
    "\n",
    "names.remove('80')\n",
    "print(names)\n",
    "\n",
    "\n",
    "# sorting list  # if we have both strin and numeric datatype in a list , then it will not sort & it throws an error\n",
    "\n",
    "names.sort()\n",
    "print(names)\n",
    "\n",
    "#sort in reverse \n",
    "\n",
    "names.sort(reverse=True)\n",
    "print(names)\n",
    "\n",
    "# Looping thr lists \n",
    "\n",
    "for items in names:\n",
    "    print(items)\n",
    "\n",
    "\n",
    "# Looping thr lists  & if we want to print teh index along with value , then we wil use enumerate on list\n",
    "\n",
    "for items in enumerate(names):\n",
    "    print(items)\n",
    "\n",
    "for index,items in enumerate(names):\n",
    "    print(index,items)\n",
    "\n",
    "for index,items in enumerate(names, start = 1): # if we want to start the index from \n",
    "    print(index,items)\n",
    "\n",
    "\n",
    "# find index for specific value\n",
    "\n",
    "print(names.index('Abd'))\n",
    "\n",
    "#Finding value in a list \n",
    "\n",
    "print('ind' in names)\n",
    "\n",
    "#splitting values from a list to a string\n",
    "\n",
    "str_names = '-'.join(names)\n",
    "print(str_names)\n",
    "\n",
    "# Creating a list from string \n",
    "\n",
    "lst_names= str_names.split('-')\n",
    "print(lst_names)\n",
    "\n",
    "\n",
    "#---TUPLES\n",
    "\n",
    "# tupleas are Mutable represented by open and close brackets () , can hold multiple data types and is ordered\n",
    "\n",
    "name_tpl= ('Aj','Sam',80)\n",
    "print(name_tpl)\n",
    "\n",
    "print(name_tpl[0]) \n",
    "\n",
    "name_tp2= ('nash','Sam',80)\n",
    "\n",
    "#combine two tuples\n",
    "#print(name_tpl.concat(name_tp2)) # erroe\n",
    "\n",
    "\n",
    "print(name_tpl + name_tp2)\n",
    "\n",
    "# SETS\n",
    "# seta are  immutable and unordered , and it wont accept duplicate values. Represented by {} - It can hold multipl edata types\n",
    "\n",
    "name_set ={'Aj','nash','dan', 'Aj',80}\n",
    "name_set2 ={'Aj','Shawn','Rod'}\n",
    "print(name_set)\n",
    "print(name_set2)\n",
    "\n",
    "#find common in two sets \n",
    "\n",
    "print(name_set.intersection(name_set2))\n",
    "\n",
    "print(name_set.difference(name_set2)) # present in first but not in second set\n",
    "\n",
    "#combine two sets \n",
    "combined_set = name_set.union(name_set2)\n",
    "print(combined_set)\n",
    "\n",
    "\n",
    "#DICTIONARIES\n",
    "\n",
    "# thsese are key value pairs ..key is unique identifier and value is the Data\n",
    "\n",
    "table = {\"name\" : 'AJ',\n",
    "            \"age\": 24,\n",
    "                \"place\" : 'india',\n",
    "                    \"courdes\": ['math','sci','phy']}\n",
    "\n",
    "print(table)\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 173,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'name': 'AJ', 'age': 24, 'place': 'india', 'courses': ['math', 'sci', 'phy']}\n"
     ]
    }
   ],
   "source": [
    "table = {\"name\" : 'AJ',\n",
    "            \"age\": 24,\n",
    "                \"place\" : 'india',\n",
    "                    \"courses\": ['math','sci','phy']}\n",
    "\n",
    "print(table)\n",
    "\n",
    "#how to access each value  base don key \n",
    "\n",
    "table['place']\n",
    "table['courses']\n",
    "\n",
    "# get keys\n",
    "table.keys()\n",
    "#get values\n",
    "table.values()\n",
    "\n",
    "#get key and value together , we use items method\n",
    "\n",
    "for key, value in table.items():\n",
    "    print( key , value)\n",
    "\n",
    "# update values in dict \n",
    "table['age']=34\n",
    "print(table)\n",
    "\n",
    "# it is tedious if we want to update dict by above method if we need to update lot of data, instaed we can use below method\n",
    "table.update({\"age\":35, \"place\":\"canada\"})\n",
    "print(table)\n",
    "\n",
    "# add data to dict \n",
    "table['lang']='English'\n",
    "print(table)\n",
    "\n",
    "#access value based on key \n",
    "#print(table['phone']) # it hrows an error because we do not have a phone key \n",
    "#we can use get method and if try to access the key that is not present , it returns None as value\n",
    "print(table.get('phone'))\n",
    "\n",
    "# we can also pass defualt vale if we try to access the key that is not prsent\n",
    "print(table.get('phone','key_not_found'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
